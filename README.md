# PaperList

## introduction
This repository contains a list of papers related to model lens and reasoning

## papers
### survey
- **Large Language Models: A Survey**. *Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, Jianfeng Gao*. arXiv preprint arXiv:2402.06196, 2024. [[pdf](https://arxiv.org/pdf/2402.06196.pdf)]

- **Challenges and applications of large language models**. *Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert*. arXiv preprint arXiv:2307.10169, 2023. [[pdf](https://arxiv.org/pdf/2307.10169.pdf)]

- **A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications**. *Sahoo, Pranab and Singh, Ayush Kumar and Saha, Sriparna and Jain, Vinija and Mondal, Samrat and Chadha, Aman*. arXiv preprint arXiv:2402.07927, 2024. [[pdf](https://arxiv.org/abs/2402.07927)]

### benchmark
- **Training VeriÔ¨Åers to Solve Math Word Problems**. *Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman*. CoRR, 2021. [[pdf](https://arxiv.org/pdf/2110.14168.pdf?curius=520), [code](https://github.com/openai/grade-school-math)]

- **Measuring mathematical problem solving with the math dataset**. *Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob*. NeurIPS, 2022. [[pdf](https://arxiv.org/pdf/2103.03874.pdf), [code](https://github.com/hendrycks/math), [dataset](https://github.com/hendrycks/math)]

- **MathScale: Scaling Instruction Tuning for Mathematical Reasoning**. *Zhengyang Tang, Xingxing Zhang, Benyou Wang, Furu Wei*. arXiv preprint arXiv:2403.02884, 2024. [[pdf](https://arxiv.org/pdf/2403.02884.pdf)]

- **AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts**. *Yifan Zhang, Yifan Luo, Yang Yuan, Andrew Chi-Chih Yao*. arXiv preprint arXiv:2402.07625, 2024. [[pdf](https://arxiv.org/pdf/2402.07625.pdf), [code](https://github.com/yifanzhang-pro/AutoMathText), [dataset](https://huggingface.co/datasets/math-ai/AutoMathText)]

- **Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models**. *Levy, Mosh and Jacoby, Alon and Goldberg, Yoav*. arXiv preprint arXiv:2402.14848, 2024. [[pdf](https://arxiv.org/pdf/2402.14848.pdf), [dataset](https://github.com/alonj/Same-Task-More-Tokens)]

### method
- **Understanding intermediate layers using linear classifier probes**. *Alain, Guillaume and Bengio, Yoshua*. ICLR, 2017. [[pdf](https://arxiv.org/pdf/1610.01644.pdf)]

- **interpreting GPT: the logit lens**. *nostalgebraist*. LessWrong, 2020. [[url](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens), [code](https://colab.research.google.com/drive/1MjdfK2srcerLrAJDRaJQKO0sUiZ-hQtA?usp=sharing)]

- **Eliciting Latent Predictions from Transformers with the Tuned Lens**. *Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and Ostrovsky, Igor and McKinney, Lev and Biderman, Stella and Steinhardt, Jacob*. CoRR, 2023. [[pdf](https://arxiv.org/pdf/2303.08112.pdf), [code](https://github.com/AlignmentResearch/tuned-lens)]

- **Overthinking the truth: Understanding how language models process false demonstrations**. *Halawi, Danny and Denain, Jean-Stanislas and Steinhardt, Jacob*. ICLR, 2024. [[pdf](https://arxiv.org/pdf/2307.09476.pdf), [code](https://github.com/dannyallover/overthinking_the_truth)]

### others
- **Chain-of-Thought Reasoning Without Prompting**. *Wang, Xuezhi and Zhou, Denny*. arXiv preprint arXiv:2402.10200, 2024. [[pdf](https://arxiv.org/pdf/2402.10200.pdf)]

- **Premise Order Matters in Reasoning with Large Language Models**. *Chen, Xinyun and Chi, Ryan A and Wang, Xuezhi and Zhou, Denny*. arXiv preprint arXiv:2402.08939, 2024. [[pdf](https://arxiv.org/pdf/2402.08939.pdf)]

- **Do Large Language Models Latently Perform Multi-Hop Reasoning?**. *Yang, Sohee and Gribovskaya, Elena and Kassner, Nora and Geva, Mor and Riedel, Sebastian*. arXiv preprint arXiv:2402.16837, 2024. [[pdf](https://arxiv.org/pdf/2402.16837v1.pdf)]

- **Residual connections encourage iterative inference**. *Jastrzebski, Stanislaw an
- d Arpit, Devansh and Ballas, Nicolas and Verma, Vikas and Che, Tong and Bengio, Yoshua*. ICLR, 2018. [[pdf](https://arxiv.org/pdf/1710.04773.pdf), [code](https://github.com/AlignmentResearch/tuned-lens)]

- **Confident adaptive language modeling**. *Schuster, Tal and Fisch, Adam and Gupta, Jai and Dehghani, Mostafa and Bahri, Dara and Tran, Vinh and Tay, Yi and Metzler, Donald*. NeurIPS, 2022. [[pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/6fac9e316a4ae75ea244ddcef1982c71-Paper-Conference.pdf), [code](https://github.com/AlignmentResearch/tuned-lens)]

- **All bark and no bite: Rogue dimensions in transformer language models obscure representational quality**. *Timkey, William and van Schijndel, Marten*. EMNLP, 2021. [[pdf](https://arxiv.org/pdf/2109.04404.pdf), [code](https://github.com/wtimkey/rogue-dimensions)]

- **Orca-Math: Unlocking the potential of SLMs in Grade School Math**. *Mitra, Arindam and Khanpour, Hamed and Rosset, Corby and Awadallah, Ahmed*. arXiv preprint arXiv:2402.14830, 2024. [[pdf](https://arxiv.org/pdf/2402.14830.pdf)]