# PaperList
## introduction
This repository contains a list of papers related to model lens

## papers
- **Eliciting Latent Predictions from Transformers with the Tuned Lens**. *Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and Ostrovsky, Igor and McKinney, Lev and Biderman, Stella and Steinhardt, Jacob*. arXiv preprint 2023. [[pdf](https://arxiv.org/pdf/2303.08112.pdf), [code](https://github.com/AlignmentResearch/tuned-lens)]

- **interpreting GPT: the logit lens**. *nostalgebraist*. LessWrong, 2020. [[url](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens), [code](https://colab.research.google.com/drive/1MjdfK2srcerLrAJDRaJQKO0sUiZ-hQtA?usp=sharing)]

- **Understanding intermediate layers using linear classifier probes**. *Alain, Guillaume and Bengio, Yoshua*. arXiv preprint arXiv:1610.01644, 2016. [[pdf](https://arxiv.org/pdf/1610.01644.pdf)]

- **Residual connections encourage iterative inference**. *Jastrzebski, Stanislaw and Arpit, Devansh and Ballas, Nicolas and Verma, Vikas and Che, Tong and Bengio, Yoshua*. arXiv preprint arXiv:1710.04773, 2017. [[pdf](https://arxiv.org/pdf/1710.04773.pdf), [code](https://github.com/AlignmentResearch/tuned-lens)]

- **Overthinking the truth: Understanding how language models process false demonstrations**. *Halawi, Danny and Denain, Jean-Stanislas and Steinhardt, Jacob*. In Submitted to The Eleventh International Conference on Learning Representations, 2023. [[pdf](https://arxiv.org/pdf/2307.09476.pdf), [code](https://github.com/dannyallover/overthinking_the_truth)]

- **Confident adaptive language modeling**. *Schuster, Tal and Fisch, Adam and Gupta, Jai and Dehghani, Mostafa and Bahri, Dara and Tran, Vinh and Tay, Yi and Metzler, Donald*. neurips, 2022.~~arXiv preprint arXiv:2207.07061, 2022.~~ [[pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/6fac9e316a4ae75ea244ddcef1982c71-Paper-Conference.pdf), [code](https://github.com/AlignmentResearch/tuned-lens)]

- **All bark and no bite: Rogue dimensions in transformer language models obscure representational quality**. *Timkey, William and van Schijndel, Marten*. arXiv preprint arXiv:2109.04404, 2021. [[pdf](https://arxiv.org/pdf/2109.04404.pdf), [code](https://github.com/wtimkey/rogue-dimensions)]
